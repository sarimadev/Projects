{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.6-py3-none-win_amd64.whl (70.9 MB)\n",
      "     --------------------------------------- 70.9/70.9 MB 16.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\sarim\\anaconda3\\lib\\site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\sarim\\anaconda3\\lib\\site-packages (from xgboost) (1.10.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.6\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and explore audio sample\n",
    "ravdess_path = 'C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24'\n",
    "crema_path = 'C:/Users/sarim/Downloads/CREMA/AudioWAV'\n",
    "tess_path= 'C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAVDESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(calm        192\n",
       " happy       192\n",
       " sad         192\n",
       " angry       192\n",
       " fear        192\n",
       " disgust     192\n",
       " surprise    192\n",
       " neutral      96\n",
       " Name: Emotion, dtype: int64,\n",
       " (1440,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a list of the audio directories in ravdess_path\n",
    "ravdess_dir_list = os.listdir(ravdess_path)\n",
    "ravdess_dir_list.sort()\n",
    "\n",
    "#extract emotion label for each audio in the created directory\n",
    "audio_emotion = []\n",
    "audio_path = []\n",
    "for dir in ravdess_dir_list:\n",
    "    actor = os.listdir(ravdess_path + '/' + dir)#iterate and extract audio files for each actor\n",
    "    for audio_file in actor:\n",
    "        part = audio_file.split('.')[0].split('-')\n",
    "        audio_emotion.append(int(part[2]))#The third index part in each audio reps the emotion label\n",
    "        audio_path.append(ravdess_path + '/' + dir + '/' + audio_file)\n",
    "\n",
    "#Insert audio_path and emotions into a DataFrame and replace numerical emotion labels with actual emotions\n",
    "ravdess_emotion_df = pd.DataFrame(audio_emotion, columns=['Emotion'])\n",
    "ravdess_emotion_df = ravdess_emotion_df.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\n",
    "audio_path_df = pd.DataFrame(audio_path, columns=['Path'])\n",
    "\n",
    "Ravdess_df = pd.concat([ravdess_emotion_df, audio_path_df], axis = 1)\n",
    "Ravdess_df.Emotion.value_counts(), Ravdess_df.Path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarim\\AppData\\Local\\Temp\\ipykernel_25924\\260657533.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>neutral</td>\n",
       "      <td>C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_14/03-01-01-01-01-02-14.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>calm</td>\n",
       "      <td>C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_14/03-01-02-01-01-01-14.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>angry</td>\n",
       "      <td>C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_12/03-01-05-01-01-02-12.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>sad</td>\n",
       "      <td>C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_03/03-01-04-02-02-02-03.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>surprise</td>\n",
       "      <td>C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_11/03-01-08-01-02-02-11.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>surprise</td>\n",
       "      <td>C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_05/03-01-08-01-01-01-05.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>happy</td>\n",
       "      <td>C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_18/03-01-03-01-01-01-18.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>disgust</td>\n",
       "      <td>C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_16/03-01-07-02-01-01-16.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>fear</td>\n",
       "      <td>C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_15/03-01-06-01-01-01-15.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>fear</td>\n",
       "      <td>C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_12/03-01-06-02-02-01-12.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Emotion  \\\n",
       "781   neutral    \n",
       "784   calm       \n",
       "689   angry      \n",
       "147   sad        \n",
       "655   surprise   \n",
       "292   surprise   \n",
       "1032  happy      \n",
       "948   disgust    \n",
       "876   fear       \n",
       "702   fear       \n",
       "\n",
       "                                                                                              Path  \n",
       "781   C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_14/03-01-01-01-01-02-14.wav  \n",
       "784   C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_14/03-01-02-01-01-01-14.wav  \n",
       "689   C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_12/03-01-05-01-01-02-12.wav  \n",
       "147   C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_03/03-01-04-02-02-02-03.wav  \n",
       "655   C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_11/03-01-08-01-02-02-11.wav  \n",
       "292   C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_05/03-01-08-01-01-01-05.wav  \n",
       "1032  C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_18/03-01-03-01-01-01-18.wav  \n",
       "948   C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_16/03-01-07-02-01-01-16.wav  \n",
       "876   C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_15/03-01-06-01-01-01-15.wav  \n",
       "702   C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_12/03-01-06-02-02-01-12.wav  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets view the full dataset\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "Ravdess_df.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREMA-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(angry      1271\n",
       " disgust    1271\n",
       " fear       1271\n",
       " happy      1271\n",
       " sad        1271\n",
       " neutral    1087\n",
       " Name: Emotion, dtype: int64,\n",
       " (7442,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a list of the audio directories in ravdess_path\n",
    "crema_dir_list = os.listdir(crema_path)\n",
    "crema_dir_list.sort()\n",
    "\n",
    "#extract emotion label for each audio in the created directory\n",
    "\n",
    "audio_emotion = []\n",
    "audio_path = []\n",
    "\n",
    "for wav in crema_dir_list:\n",
    "    audio_path.append(crema_path + '/' + wav)\n",
    "    part=wav.split('_')\n",
    "    if part[2] == 'SAD':\n",
    "        audio_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        audio_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        audio_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        audio_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        audio_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        audio_emotion.append('neutral')\n",
    "    else:\n",
    "        audio_emotion.append('Unknown')\n",
    "\n",
    "# dataframe for emotion of files\n",
    "crema_emotion_df = pd.DataFrame(audio_emotion, columns=['Emotion'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "audio_path_df = pd.DataFrame(audio_path, columns=['Path'])\n",
    "Crema_D_df = pd.concat([crema_emotion_df, audio_path_df], axis=1)\n",
    "\n",
    "Crema_D_df.Emotion.value_counts(), Crema_D_df.Path.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarim\\AppData\\Local\\Temp\\ipykernel_25924\\1053867059.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>angry</td>\n",
       "      <td>C:/Users/sarim/Downloads/CREMA/AudioWAV/1007_DFA_ANG_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6357</th>\n",
       "      <td>sad</td>\n",
       "      <td>C:/Users/sarim/Downloads/CREMA/AudioWAV/1078_TAI_SAD_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>disgust</td>\n",
       "      <td>C:/Users/sarim/Downloads/CREMA/AudioWAV/1040_WSI_DIS_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>disgust</td>\n",
       "      <td>C:/Users/sarim/Downloads/CREMA/AudioWAV/1013_TSI_DIS_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5460</th>\n",
       "      <td>happy</td>\n",
       "      <td>C:/Users/sarim/Downloads/CREMA/AudioWAV/1067_TIE_HAP_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4689</th>\n",
       "      <td>angry</td>\n",
       "      <td>C:/Users/sarim/Downloads/CREMA/AudioWAV/1058_ITS_ANG_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6255</th>\n",
       "      <td>happy</td>\n",
       "      <td>C:/Users/sarim/Downloads/CREMA/AudioWAV/1077_IWL_HAP_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>fear</td>\n",
       "      <td>C:/Users/sarim/Downloads/CREMA/AudioWAV/1032_IEO_FEA_LO.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>fear</td>\n",
       "      <td>C:/Users/sarim/Downloads/CREMA/AudioWAV/1030_TSI_FEA_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6111</th>\n",
       "      <td>neutral</td>\n",
       "      <td>C:/Users/sarim/Downloads/CREMA/AudioWAV/1075_TAI_NEU_XX.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Emotion                                                         Path\n",
       "491   angry    C:/Users/sarim/Downloads/CREMA/AudioWAV/1007_DFA_ANG_XX.wav\n",
       "6357  sad      C:/Users/sarim/Downloads/CREMA/AudioWAV/1078_TAI_SAD_XX.wav\n",
       "3256  disgust  C:/Users/sarim/Downloads/CREMA/AudioWAV/1040_WSI_DIS_XX.wav\n",
       "1042  disgust  C:/Users/sarim/Downloads/CREMA/AudioWAV/1013_TSI_DIS_XX.wav\n",
       "5460  happy    C:/Users/sarim/Downloads/CREMA/AudioWAV/1067_TIE_HAP_XX.wav\n",
       "4689  angry    C:/Users/sarim/Downloads/CREMA/AudioWAV/1058_ITS_ANG_XX.wav\n",
       "6255  happy    C:/Users/sarim/Downloads/CREMA/AudioWAV/1077_IWL_HAP_XX.wav\n",
       "2536  fear     C:/Users/sarim/Downloads/CREMA/AudioWAV/1032_IEO_FEA_LO.wav\n",
       "2431  fear     C:/Users/sarim/Downloads/CREMA/AudioWAV/1030_TSI_FEA_XX.wav\n",
       "6111  neutral  C:/Users/sarim/Downloads/CREMA/AudioWAV/1075_TAI_NEU_XX.wav"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets view the full dataset\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "Crema_D_df.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(fear        400\n",
       " surprise    400\n",
       " sad         400\n",
       " angry       400\n",
       " disgust     400\n",
       " happy       400\n",
       " neutral     400\n",
       " Name: Emotion, dtype: int64,\n",
       " (2800,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tess_dir_list = os.listdir(tess_path)\n",
    "tess_dir_list.sort()\n",
    "\n",
    "audio_emotion = []\n",
    "audio_path = []\n",
    "\n",
    "for dir in tess_dir_list:\n",
    "    folders = os.listdir(tess_path + '/' + dir)\n",
    "    for wav in folders:\n",
    "        part = wav.split('.')[0]\n",
    "        part = part.split('_')[2]\n",
    "        if part=='ps':\n",
    "            audio_emotion.append('surprise')\n",
    "        else:\n",
    "            audio_emotion.append(part)\n",
    "        audio_path.append(tess_path + '/' + dir + '/' + wav)\n",
    "\n",
    "tess_emotion_df = pd.DataFrame(audio_emotion, columns=['Emotion'])\n",
    "\n",
    "audio_path_df = pd.DataFrame(audio_path, columns=['Path'])\n",
    "Tess_df = pd.concat([tess_emotion_df, audio_path_df], axis=1)\n",
    "Tess_df.Emotion.value_counts(), Tess_df.Path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarim\\AppData\\Local\\Temp\\ipykernel_25924\\4236753067.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fear</td>\n",
       "      <td>C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/OAF_Fear/OAF_back_fear.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/OAF_Fear/OAF_bar_fear.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/OAF_Fear/OAF_base_fear.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fear</td>\n",
       "      <td>C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/OAF_Fear/OAF_bath_fear.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fear</td>\n",
       "      <td>C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/OAF_Fear/OAF_bean_fear.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotion  \\\n",
       "0  fear     \n",
       "1  fear     \n",
       "2  fear     \n",
       "3  fear     \n",
       "4  fear     \n",
       "\n",
       "                                                                                                                                     Path  \n",
       "0  C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/OAF_Fear/OAF_back_fear.wav  \n",
       "1  C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/OAF_Fear/OAF_bar_fear.wav   \n",
       "2  C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/OAF_Fear/OAF_base_fear.wav  \n",
       "3  C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/OAF_Fear/OAF_bath_fear.wav  \n",
       "4  C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/OAF_Fear/OAF_bean_fear.wav  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets view the full dataset\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "Tess_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA CONCATENATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_01/03-01-01-01-01-01-01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_01/03-01-01-01-01-02-01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_01/03-01-01-01-02-01-01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_01/03-01-01-01-02-02-01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>calm</td>\n",
       "      <td>C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_01/03-01-02-01-01-01-01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11677</th>\n",
       "      <td>sad</td>\n",
       "      <td>C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_sad/YAF_witch_sad.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11678</th>\n",
       "      <td>sad</td>\n",
       "      <td>C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_sad/YAF_yearn_sad.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11679</th>\n",
       "      <td>sad</td>\n",
       "      <td>C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_sad/YAF_yes_sad.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11680</th>\n",
       "      <td>sad</td>\n",
       "      <td>C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_sad/YAF_young_sad.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11681</th>\n",
       "      <td>sad</td>\n",
       "      <td>C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_sad/YAF_youth_sad.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11682 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Emotion  \\\n",
       "0      neutral   \n",
       "1      neutral   \n",
       "2      neutral   \n",
       "3      neutral   \n",
       "4      calm      \n",
       "...     ...      \n",
       "11677  sad       \n",
       "11678  sad       \n",
       "11679  sad       \n",
       "11680  sad       \n",
       "11681  sad       \n",
       "\n",
       "                                                                                                                                        Path  \n",
       "0      C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_01/03-01-01-01-01-01-01.wav                                           \n",
       "1      C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_01/03-01-01-01-01-02-01.wav                                           \n",
       "2      C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_01/03-01-01-01-02-01-01.wav                                           \n",
       "3      C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_01/03-01-01-01-02-02-01.wav                                           \n",
       "4      C:/Users/sarim/Downloads/RAVDESS/audio_speech_actors_01-24/Actor_01/03-01-02-01-01-01-01.wav                                           \n",
       "...                                                                                             ...                                           \n",
       "11677  C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_sad/YAF_witch_sad.wav  \n",
       "11678  C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_sad/YAF_yearn_sad.wav  \n",
       "11679  C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_sad/YAF_yes_sad.wav    \n",
       "11680  C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_sad/YAF_young_sad.wav  \n",
       "11681  C:/Users/sarim/Downloads/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/YAF_sad/YAF_youth_sad.wav  \n",
       "\n",
       "[11682 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_audio_df = pd.concat([Ravdess_df, Crema_D_df, Tess_df], axis=0,ignore_index=True) #concat all df except savee\n",
    "main_audio_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIGNAL PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11682 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11682/11682 [00:37<00:00, 307.58it/s]\n"
     ]
    }
   ],
   "source": [
    "working_df = main_audio_df.copy()\n",
    "working_df[\"Signal\"] = None\n",
    "working_df[\"Sampling Rate\"] = None\n",
    "\n",
    "for row, path in tqdm(enumerate(working_df['Path']), total=len(working_df)):\n",
    "    signal, rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "    working_df.at[row, \"Signal\"] = signal\n",
    "    working_df.at[row, \"Sampling Rate\"] = rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Noise addition, time and pitch shifting\n",
    "def noise(signal):\n",
    "    noisy_audio = signal+0.035*np.random.uniform()*np.amax(signal)*np.random.normal(size=signal.shape[0])\n",
    "    return noisy_audio\n",
    "\n",
    "def pitch(signal, sample_rate, n_steps=0.5):\n",
    "    shifted_audio = librosa.effects.pitch_shift(signal, sr=sample_rate, n_steps=0.5)\n",
    "    return shifted_audio\n",
    "\n",
    "def time_stretch(signal, rate=1.2):\n",
    "    stretched_audio = librosa.effects.time_stretch(signal, rate=1.2)\n",
    "    return  stretched_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create DFs for the each augumentation technique\n",
    "noise_df = working_df.copy()\n",
    "pitch_stretch_df = working_df.copy()\n",
    "time_stretch_df = working_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11682/11682 [00:15<00:00, 742.48it/s]\n"
     ]
    }
   ],
   "source": [
    "noise_df[\"Signal\"] = working_df[\"Signal\"].progress_apply(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11682 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11682/11682 [17:31<00:00, 11.11it/s]\n"
     ]
    }
   ],
   "source": [
    "#apply time_stretch to audio signal\n",
    "time_stretch_df[\"Signal\"] = working_df[\"Signal\"].progress_apply(time_stretch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11682 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11682/11682 [26:03<00:00,  7.47it/s]\n"
     ]
    }
   ],
   "source": [
    "pitch_stretch_df[\"Signal\"] = time_stretch_df.progress_apply(lambda x : pitch(x['Signal'], 22050), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35046, 4)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concat all augmented audio\n",
    "\n",
    "augmented_df = pd.concat([working_df, noise_df, pitch_stretch_df], axis=0,ignore_index=True)\n",
    "augmented_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio, sample_rate):\n",
    "   \n",
    "    try:\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sample_rate)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=sample_rate)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y=audio)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", e)\n",
    "        return None \n",
    "     \n",
    "    return {\"mfccs_mean\":np.mean(mfccs,axis=1), \"mfccs_std\":np.std(mfccs,axis=1),\n",
    "            \"chroma_mean\":np.mean(chroma,axis=1), \"chroma_std\":np.std(chroma,axis=1),\n",
    "            \"spectral_contrast_mean\":np.mean(spectral_contrast,axis=1), \"spectral_contrast_std\":np.std(spectral_contrast,axis=1),\n",
    "            \"tonnetz_mean\":np.mean(tonnetz,axis=1), \"tonnetz_std\":np.std(tonnetz,axis=1),\n",
    "            \"zcr_mean\":np.mean(zcr,axis=1), \"zcr_std\":np.std(zcr,axis=1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35046 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35046/35046 [3:10:34<00:00,  3.06it/s]  \n"
     ]
    }
   ],
   "source": [
    "features_df = augmented_df.copy()\n",
    "features_df = pd.concat([features_df.drop(['Signal'], axis=1), features_df['Signal'].progress_apply(lambda x: pd.Series(extract_features(x, sample_rate=22050)))], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# saving the dataframe\n",
    "features_df.to_csv('Extracted_features.csv')\n",
    "features_df.to_pickle('Extracted_features.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mel_spectrogram_features(signal, sr):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sr).T\n",
    "    return np.mean(mel_spectrogram, axis=0), np.std(mel_spectrogram, axis=0)\n",
    "\n",
    "mel_features_df = augmented_df.copy()\n",
    "mel_features_df[['mel_spectrogram_mean', 'mel_spectrogram_std']] = mel_features_df['Signal'].progress_apply(\n",
    "    lambda x: pd.Series(compute_mel_spectrogram_features(x, sr=22050))\n",
    ")\n",
    "\n",
    "\n",
    "mel_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's concatenate the mel features to the features_df\n",
    "updated_features_df = pd.concat([features_df, mel_features_df[['mel_spectrogram_mean', 'mel_spectrogram_std']]], axis=1)\n",
    "\n",
    "updated_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save updated features\n",
    "updated_features_df.to_csv('Extracted_features(update).csv')\n",
    "updated_features_df.to_pickle('Extracted_features(update).pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep features for feature selection\n",
    "# Create new dataframe to hold flattened features\n",
    "flattened_features_df = updated_features_df.copy()\n",
    "\n",
    "# Define function to flatten a feature\n",
    "def flatten_feature(df, column):\n",
    "    flat_df = pd.DataFrame(df[column].to_list(), index=df.index)\n",
    "    flat_df.columns = [f'{column}_{i+1}' for i in range(flat_df.shape[1])]\n",
    "    return df.drop(column, axis=1).join(flat_df)\n",
    "\n",
    "# List of columns to flatten\n",
    "columns_to_flatten = ['mfccs_mean', 'mfccs_std', 'chroma_mean', 'chroma_std',\n",
    "                      'spectral_contrast_mean', 'spectral_contrast_std',\n",
    "                      'tonnetz_mean', 'tonnetz_std', 'zcr_mean', 'zcr_std',\n",
    "                      'mel_spectrogram_mean', 'mel_spectrogram_std']\n",
    "\n",
    "# Flatten each column\n",
    "for column in columns_to_flatten:\n",
    "    flattened_features_df = flatten_feature(flattened_features_df, column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35046, 391)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save flattened df\n",
    "flattened_features_df.to_csv('Flattened_features.csv')\n",
    "flattened_features_df.to_pickle('Flattened_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>mfccs_mean_1</th>\n",
       "      <th>mfccs_mean_2</th>\n",
       "      <th>mfccs_mean_3</th>\n",
       "      <th>mfccs_mean_4</th>\n",
       "      <th>mfccs_mean_5</th>\n",
       "      <th>mfccs_mean_6</th>\n",
       "      <th>mfccs_mean_7</th>\n",
       "      <th>mfccs_mean_8</th>\n",
       "      <th>mfccs_mean_9</th>\n",
       "      <th>...</th>\n",
       "      <th>mel_spectrogram_std_119</th>\n",
       "      <th>mel_spectrogram_std_120</th>\n",
       "      <th>mel_spectrogram_std_121</th>\n",
       "      <th>mel_spectrogram_std_122</th>\n",
       "      <th>mel_spectrogram_std_123</th>\n",
       "      <th>mel_spectrogram_std_124</th>\n",
       "      <th>mel_spectrogram_std_125</th>\n",
       "      <th>mel_spectrogram_std_126</th>\n",
       "      <th>mel_spectrogram_std_127</th>\n",
       "      <th>mel_spectrogram_std_128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>-645.880737</td>\n",
       "      <td>72.619637</td>\n",
       "      <td>0.841305</td>\n",
       "      <td>16.399446</td>\n",
       "      <td>10.241591</td>\n",
       "      <td>0.653901</td>\n",
       "      <td>-4.328002</td>\n",
       "      <td>-4.258932</td>\n",
       "      <td>-14.575824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>-634.663269</td>\n",
       "      <td>72.324066</td>\n",
       "      <td>-2.998578</td>\n",
       "      <td>20.209740</td>\n",
       "      <td>10.674217</td>\n",
       "      <td>-1.151011</td>\n",
       "      <td>-2.813592</td>\n",
       "      <td>-7.923956</td>\n",
       "      <td>-16.231958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>-643.285645</td>\n",
       "      <td>74.262268</td>\n",
       "      <td>-1.228256</td>\n",
       "      <td>16.242317</td>\n",
       "      <td>5.608830</td>\n",
       "      <td>0.250306</td>\n",
       "      <td>-4.034254</td>\n",
       "      <td>-8.218330</td>\n",
       "      <td>-14.603807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>-644.624451</td>\n",
       "      <td>69.160400</td>\n",
       "      <td>3.003006</td>\n",
       "      <td>16.580229</td>\n",
       "      <td>7.867353</td>\n",
       "      <td>2.587296</td>\n",
       "      <td>-2.231727</td>\n",
       "      <td>-8.238157</td>\n",
       "      <td>-12.929442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>calm</td>\n",
       "      <td>-652.413757</td>\n",
       "      <td>87.086311</td>\n",
       "      <td>3.265245</td>\n",
       "      <td>20.345898</td>\n",
       "      <td>10.620335</td>\n",
       "      <td>1.831458</td>\n",
       "      <td>-5.369885</td>\n",
       "      <td>-6.077728</td>\n",
       "      <td>-14.500923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 389 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion  mfccs_mean_1  mfccs_mean_2  mfccs_mean_3  mfccs_mean_4  \\\n",
       "0  neutral -645.880737    72.619637     0.841305      16.399446      \n",
       "1  neutral -634.663269    72.324066    -2.998578      20.209740      \n",
       "2  neutral -643.285645    74.262268    -1.228256      16.242317      \n",
       "3  neutral -644.624451    69.160400     3.003006      16.580229      \n",
       "4  calm    -652.413757    87.086311     3.265245      20.345898      \n",
       "\n",
       "   mfccs_mean_5  mfccs_mean_6  mfccs_mean_7  mfccs_mean_8  mfccs_mean_9  ...  \\\n",
       "0  10.241591     0.653901     -4.328002     -4.258932     -14.575824     ...   \n",
       "1  10.674217    -1.151011     -2.813592     -7.923956     -16.231958     ...   \n",
       "2  5.608830      0.250306     -4.034254     -8.218330     -14.603807     ...   \n",
       "3  7.867353      2.587296     -2.231727     -8.238157     -12.929442     ...   \n",
       "4  10.620335     1.831458     -5.369885     -6.077728     -14.500923     ...   \n",
       "\n",
       "   mel_spectrogram_std_119  mel_spectrogram_std_120  mel_spectrogram_std_121  \\\n",
       "0  0.000013                 0.000014                 0.000009                  \n",
       "1  0.000031                 0.000022                 0.000019                  \n",
       "2  0.000049                 0.000050                 0.000045                  \n",
       "3  0.000065                 0.000080                 0.000069                  \n",
       "4  0.000034                 0.000028                 0.000022                  \n",
       "\n",
       "   mel_spectrogram_std_122  mel_spectrogram_std_123  mel_spectrogram_std_124  \\\n",
       "0  0.000005                 0.000006                 0.000017                  \n",
       "1  0.000024                 0.000025                 0.000051                  \n",
       "2  0.000064                 0.000094                 0.000130                  \n",
       "3  0.000095                 0.000453                 0.000181                  \n",
       "4  0.000072                 0.000030                 0.000015                  \n",
       "\n",
       "   mel_spectrogram_std_125  mel_spectrogram_std_126  mel_spectrogram_std_127  \\\n",
       "0  0.000028                 0.000021                 0.000019                  \n",
       "1  0.000036                 0.000045                 0.000023                  \n",
       "2  0.000248                 0.000141                 0.000069                  \n",
       "3  0.000155                 0.000175                 0.000102                  \n",
       "4  0.000018                 0.000019                 0.000015                  \n",
       "\n",
       "   mel_spectrogram_std_128  \n",
       "0  0.000002                 \n",
       "1  0.000001                 \n",
       "2  0.000007                 \n",
       "3  0.000007                 \n",
       "4  0.000001                 \n",
       "\n",
       "[5 rows x 389 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DROP PATH COLUMN FOR MODELING\n",
    "model_df = flattened_features_df.copy()\n",
    "model_df.drop(columns=['Path','Sampling Rate'],inplace=True)\n",
    "\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35046, 389)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARING DATA FOR MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X(predictors) and Y(target)\n",
    "X = model_df.drop(columns=['Emotion'])\n",
    "y = model_df['Emotion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding the target variable for traditional ML models\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# One-hot encoding the target variable for deep learning models\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation-test split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=10)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=10)\n",
    "\n",
    "X_temp_dl, X_test_dl, y_temp_dl, y_test_dl = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "X_train_dl, X_val_dl, y_train_dl, y_val_dl = train_test_split(X_temp_dl, y_temp_dl, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train.shape, X_train_dl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Scaling the features for traditional models\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[0;32m      3\u001b[0m X_train \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(X_train)\n\u001b[0;32m      4\u001b[0m X_test \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "# Scaling the features for traditional models\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Scaling the features for deep learning models\n",
    "X_train_dl = scaler.fit_transform(X_train_dl)\n",
    "X_test_dl = scaler.transform(X_test_dl)\n",
    "X_val_dl = scaler.transform(X_val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22428, 388, 1), (7010, 388, 1), (5608, 388, 1))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a pipeline for preprocessing the deep learning data\n",
    "# This step is necessary because the input to a CNN must be a 3D array (samples, timesteps, features)\n",
    "# Reshaping the data to be suitable for CNN\n",
    "X_train_dl = X_train_dl.reshape(X_train_dl.shape[0], X_train_dl.shape[1], 1)\n",
    "X_test_dl = X_test_dl.reshape(X_test_dl.shape[0], X_test_dl.shape[1], 1)\n",
    "X_val_dl = X_val_dl.reshape(X_val_dl.shape[0], X_val_dl.shape[1], 1)\n",
    "\n",
    "X_train_dl.shape, X_test_dl.shape, X_val_dl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((22428, 8), (7010, 8))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train_dl[0])\n",
    "print(y_test_dl[0])\n",
    "y_train_dl.shape, y_test_dl.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BASELINES\n",
    "\n",
    "The purpose of a baseline is to set a reasonably low bar to improve upon and for the purpose of this project to see if there might be a need\n",
    "for feature selection. \n",
    "We'll be using Gaussian Naive Bayes and Decision Trees as baselines since we are dealing with continuous audio features extracted from sound clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Gaussian Naive Bayes ---\n",
      "Accuracy: 0.269472182596291\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.27      0.38      1103\n",
      "           1       0.15      0.93      0.25       123\n",
      "           2       0.15      0.11      0.13      1082\n",
      "           3       0.43      0.08      0.14      1113\n",
      "           4       0.30      0.14      0.19      1101\n",
      "           5       0.25      0.84      0.38       973\n",
      "           6       0.27      0.10      0.14      1170\n",
      "           7       0.30      0.54      0.39       345\n",
      "\n",
      "    accuracy                           0.27      7010\n",
      "   macro avg       0.31      0.38      0.25      7010\n",
      "weighted avg       0.34      0.27      0.23      7010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#BASELINES\n",
    "# Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_gnb = gnb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('--- Gaussian Naive Bayes ---')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred_gnb))\n",
    "print(classification_report(y_test, y_pred_gnb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Decision Tree ---\n",
      "Accuracy: 0.7601997146932953\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83      1103\n",
      "           1       0.79      0.76      0.78       123\n",
      "           2       0.72      0.71      0.71      1082\n",
      "           3       0.74      0.73      0.73      1113\n",
      "           4       0.73      0.75      0.74      1101\n",
      "           5       0.74      0.73      0.73       973\n",
      "           6       0.76      0.80      0.78      1170\n",
      "           7       0.84      0.83      0.83       345\n",
      "\n",
      "    accuracy                           0.76      7010\n",
      "   macro avg       0.77      0.77      0.77      7010\n",
      "weighted avg       0.76      0.76      0.76      7010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('\\n--- Decision Tree ---')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred_dt))\n",
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the Decision Tree classifier are quite promising with an accuracy of ~76%. It looks like the features are already capturing a good amount of the underlying structure of the data.\n",
    "\n",
    "Gaussian Naive Bayes did not perform as well, which isn't surprising. Naive Bayes makes a strong assumption about the independence of features, which might not hold in this case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELLING\n",
    "\n",
    "Traditional Models\n",
    "\n",
    "Support Vector Machine (SVM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.71      1103\n",
      "           1       0.34      0.67      0.45       123\n",
      "           2       0.43      0.53      0.48      1082\n",
      "           3       0.70      0.37      0.49      1113\n",
      "           4       0.55      0.52      0.54      1101\n",
      "           5       0.54      0.43      0.48       973\n",
      "           6       0.52      0.71      0.60      1170\n",
      "           7       0.78      0.79      0.79       345\n",
      "\n",
      "    accuracy                           0.56      7010\n",
      "   macro avg       0.57      0.59      0.57      7010\n",
      "weighted avg       0.58      0.56      0.56      7010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "svm = SVC()\n",
    "\n",
    "# Fit on the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "svm_preds = svm.predict(X_test)\n",
    "\n",
    "# Generate and print the classification report\n",
    "svm_report = classification_report(y_test, svm_preds)\n",
    "print('SVM:', svm_report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.76      0.72      1103\n",
      "           1       0.82      0.89      0.85       123\n",
      "           2       0.50      0.63      0.56      1082\n",
      "           3       0.58      0.54      0.56      1113\n",
      "           4       0.66      0.56      0.61      1101\n",
      "           5       0.63      0.57      0.59       973\n",
      "           6       0.72      0.68      0.70      1170\n",
      "           7       0.94      0.93      0.94       345\n",
      "\n",
      "    accuracy                           0.64      7010\n",
      "   macro avg       0.69      0.69      0.69      7010\n",
      "weighted avg       0.65      0.64      0.64      7010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit on the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "knn_preds = knn.predict(X_test)\n",
    "\n",
    "# Generate and print the classification report\n",
    "knn_report = classification_report(y_test, knn_preds)\n",
    "print('KNN:', knn_report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89      1103\n",
      "           1       0.80      0.89      0.84       123\n",
      "           2       0.80      0.80      0.80      1082\n",
      "           3       0.91      0.75      0.82      1113\n",
      "           4       0.83      0.84      0.84      1101\n",
      "           5       0.79      0.82      0.81       973\n",
      "           6       0.77      0.88      0.82      1170\n",
      "           7       0.95      0.95      0.95       345\n",
      "\n",
      "    accuracy                           0.84      7010\n",
      "   macro avg       0.84      0.85      0.85      7010\n",
      "weighted avg       0.84      0.84      0.84      7010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Fit on the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "rf_preds = rf.predict(X_test)\n",
    "\n",
    "# Generate and print the classification report\n",
    "rf_report = classification_report(y_test, rf_preds)\n",
    "print('Random Forest:', rf_report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91      1103\n",
      "           1       0.94      0.95      0.95       123\n",
      "           2       0.82      0.84      0.83      1082\n",
      "           3       0.88      0.81      0.84      1113\n",
      "           4       0.86      0.86      0.86      1101\n",
      "           5       0.83      0.84      0.83       973\n",
      "           6       0.84      0.89      0.86      1170\n",
      "           7       0.98      0.96      0.97       345\n",
      "\n",
      "    accuracy                           0.86      7010\n",
      "   macro avg       0.88      0.88      0.88      7010\n",
      "weighted avg       0.87      0.86      0.86      7010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Fit on the training data\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "xgb_preds = xgb.predict(X_test)\n",
    "\n",
    "# Generate and print the classification report\n",
    "xgb_report = classification_report(y_test, xgb_preds)\n",
    "print('XGBoost:', xgb_report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72      1103\n",
      "           1       0.69      0.77      0.73       123\n",
      "           2       0.50      0.57      0.53      1082\n",
      "           3       0.60      0.50      0.55      1113\n",
      "           4       0.57      0.54      0.55      1101\n",
      "           5       0.59      0.55      0.57       973\n",
      "           6       0.59      0.68      0.63      1170\n",
      "           7       0.77      0.80      0.78       345\n",
      "\n",
      "    accuracy                           0.60      7010\n",
      "   macro avg       0.63      0.64      0.63      7010\n",
      "weighted avg       0.61      0.60      0.60      7010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit on the training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "lr_preds = lr.predict(X_test)\n",
    "\n",
    "# Generate and print the classification report\n",
    "lr_report = classification_report(y_test, lr_preds)\n",
    "print('Logistic Regression:', lr_report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VALIDATION\n",
    "\n",
    "This will give a sense of how the model's performance varies across different subsets of the validation set. High variance across subsets can indicate a high-variance (overfitting) model, while consistent but poor performance can indicate a high-bias (underfitting) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM mean cross-validation accuracy: 0.5557580115559595\n",
      "KNN mean cross-validation accuracy: 0.7032190525773975\n",
      "Random Forest mean cross-validation accuracy: 0.8829790391770598\n",
      "XGBoost mean cross-validation accuracy: 0.8981024560026654\n",
      "Logistic Regression mean cross-validation accuracy: 0.6018683661052051\n"
     ]
    }
   ],
   "source": [
    "# Calculate cross-validation accuracy for each model\n",
    "svm_scores = cross_val_score(svm, X, y, cv=5)\n",
    "knn_scores = cross_val_score(knn, X, y, cv=5)\n",
    "rf_scores = cross_val_score(rf, X, y, cv=5)\n",
    "xgb_scores = cross_val_score(xgb, X, y_encoded, cv=5)\n",
    "lr_scores = cross_val_score(lr, X, y, cv=5)\n",
    "\n",
    "# Print mean cross-validation accuracy for each model\n",
    "print('SVM mean cross-validation accuracy:', np.mean(svm_scores))\n",
    "print('KNN mean cross-validation accuracy:', np.mean(knn_scores))\n",
    "print('Random Forest mean cross-validation accuracy:', np.mean(rf_scores))\n",
    "print('XGBoost mean cross-validation accuracy:', np.mean(xgb_scores))\n",
    "print('Logistic Regression mean cross-validation accuracy:', np.mean(lr_scores))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, none of the models seem to be severely overfitting. The SVM model might be underfitting, though, since it has the lowest performance metrics. One common sign of overfitting is when your model performs well on the training set (high precision and recall) but poorly during cross-validation (low mean accuracy), but that doesn't seem to be the case here.\n",
    "\n",
    "The Random Forest and XGBoost models seem to be the best performers on this task based on the provided metrics. They both have high f1-scores and cross-validation accuracies, suggesting that they are generalizing well to unseen data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "877/877 [==============================] - 203s 231ms/step - loss: 1.6615 - accuracy: 0.3687 - val_loss: 1.3371 - val_accuracy: 0.4976\n",
      "Epoch 2/20\n",
      "877/877 [==============================] - 177s 202ms/step - loss: 1.3561 - accuracy: 0.4563 - val_loss: 1.1934 - val_accuracy: 0.5415\n",
      "Epoch 3/20\n",
      "877/877 [==============================] - 184s 210ms/step - loss: 1.2725 - accuracy: 0.4903 - val_loss: 1.1424 - val_accuracy: 0.5398\n",
      "Epoch 4/20\n",
      "877/877 [==============================] - 182s 207ms/step - loss: 1.2271 - accuracy: 0.5092 - val_loss: 1.1247 - val_accuracy: 0.5525\n",
      "Epoch 5/20\n",
      "877/877 [==============================] - 194s 221ms/step - loss: 1.1966 - accuracy: 0.5166 - val_loss: 1.1071 - val_accuracy: 0.5588\n",
      "Epoch 6/20\n",
      "877/877 [==============================] - 213s 243ms/step - loss: 1.1755 - accuracy: 0.5292 - val_loss: 1.0969 - val_accuracy: 0.5542\n",
      "Epoch 7/20\n",
      "877/877 [==============================] - 226s 257ms/step - loss: 1.1588 - accuracy: 0.5347 - val_loss: 1.0888 - val_accuracy: 0.5729\n",
      "Epoch 8/20\n",
      "877/877 [==============================] - 206s 235ms/step - loss: 1.1458 - accuracy: 0.5389 - val_loss: 1.0765 - val_accuracy: 0.5802\n",
      "Epoch 9/20\n",
      "877/877 [==============================] - 222s 253ms/step - loss: 1.1373 - accuracy: 0.5438 - val_loss: 1.0623 - val_accuracy: 0.5850\n",
      "Epoch 10/20\n",
      "877/877 [==============================] - 227s 258ms/step - loss: 1.1249 - accuracy: 0.5503 - val_loss: 1.0404 - val_accuracy: 0.5923\n",
      "Epoch 11/20\n",
      "877/877 [==============================] - 206s 235ms/step - loss: 1.1134 - accuracy: 0.5533 - val_loss: 1.0411 - val_accuracy: 0.5904\n",
      "Epoch 12/20\n",
      "877/877 [==============================] - 231s 263ms/step - loss: 1.1135 - accuracy: 0.5565 - val_loss: 1.0446 - val_accuracy: 0.5969\n",
      "Epoch 13/20\n",
      "877/877 [==============================] - 189s 216ms/step - loss: 1.1051 - accuracy: 0.5602 - val_loss: 1.0259 - val_accuracy: 0.5979\n",
      "Epoch 14/20\n",
      "877/877 [==============================] - 225s 257ms/step - loss: 1.0945 - accuracy: 0.5625 - val_loss: 1.0298 - val_accuracy: 0.5947\n",
      "Epoch 15/20\n",
      "877/877 [==============================] - 243s 277ms/step - loss: 1.0926 - accuracy: 0.5671 - val_loss: 1.0166 - val_accuracy: 0.5996\n",
      "Epoch 16/20\n",
      "877/877 [==============================] - 195s 223ms/step - loss: 1.0801 - accuracy: 0.5734 - val_loss: 1.0190 - val_accuracy: 0.6061\n",
      "Epoch 17/20\n",
      "877/877 [==============================] - 210s 239ms/step - loss: 1.0816 - accuracy: 0.5685 - val_loss: 1.0078 - val_accuracy: 0.6029\n",
      "Epoch 18/20\n",
      "877/877 [==============================] - 229s 261ms/step - loss: 1.0736 - accuracy: 0.5730 - val_loss: 1.0194 - val_accuracy: 0.6031\n",
      "Epoch 19/20\n",
      "877/877 [==============================] - 220s 250ms/step - loss: 1.0684 - accuracy: 0.5737 - val_loss: 0.9942 - val_accuracy: 0.6187\n",
      "Epoch 20/20\n",
      "877/877 [==============================] - 183s 209ms/step - loss: 1.0618 - accuracy: 0.5768 - val_loss: 1.0153 - val_accuracy: 0.5942\n"
     ]
    }
   ],
   "source": [
    "# Number of output classes\n",
    "num_classes = y_train_dl.shape[1] \n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(X_train_dl.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train_dl, y_train_dl, epochs=20, validation_data=(X_test_dl, y_test_dl))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETER TUNING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POST-MODEL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT LOSS AND ACCURACY PERCENTAGE ON TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACTUAL VS PREDICTED VALUES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
